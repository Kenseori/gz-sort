Нуждается в заголовках zlib и, вероятно, строится только на GNU/Linux.

    use: gz-sort [-u] [-S n] [-P n] source.gz dest.gz

    опции:
       -h: help
       -u: unique
       -S n: size of presort, supports k/M/G suffix
             a traditional in-memory sort (default n=1M)
       -P n: use multiple threads (experimental, default disabled)
       -T: pass through (debugging/benchmarks)

    грубо оценивая время выполнения:
        time gzip -dc data.gz | gzip > /dev/null
        unthreaded: seconds * entropy * (log2(uncompressed_size/S)+2)
        (where 'entropy' is a fudge-factor between 1.5 for an
        already sorted file and 3 for a shuffled file)
        S and P are the corresponding settings
        multithreaded: maybe unthreaded/sqrt(P) ?

    предполагаемое использование диска:
        2x source.gz


### Минимальные требования для сортировки терабайта:

* 4 МБ оперативной памяти (да, мегабайт)
* свободное место на диске, равное удвоенному объему сжатого source.gz


### Известные ошибки, которые необходимо исправить

Напишите мне, если вы используете gz-sort и любое из этих упущений вызывает у вас проблемы. Если уж на то пошло, напишите мне, если вы тоже найдете что-то, чего нет в этом списке.

* Не строится на системах, отличных от gnu.
* Sqrt (потоки) - ужасное соотношение.
* Нет поддержки несжатых потоков stdin.
* Прерывается, если длина строки превышает размер буфера.
* Отсутствует вся обработка ошибок.
* Уродливый код с множеством способов рефакторинга.
* В выходных данных могут использоваться предсказуемые сбросы.


### Настройки производительности, которые стоит попробовать

* Профиль!
* Распараллелить окончательное n-стороннее слияние. Для этого потребуется добавить IPC.
* Отфильтровать уникальные строки во время предыдущих проходов.
* Попробуйте zlib-ng, примерно половина процессорного времени тратится на (отмену) gzipping.
* Улучшите оценку памяти, она снижается, и это вредит предварительной сортировке.
* Поиск на основе байтов вместо подсчета строк.


